{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshaykumarmundrathi/GCN/blob/main/GCN_Task3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified GCN, according to Task 1.1:"
      ],
      "metadata": {
        "id": "Pz3EgQ9fMECp"
      },
      "id": "Pz3EgQ9fMECp"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip GCN_export.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7q4kTf7Ihp6",
        "outputId": "66429eb9-ace8-447f-cdc6-66b4d94fb05c"
      },
      "id": "G7q4kTf7Ihp6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  GCN_export.zip\n",
            "   creating: data/\n",
            "   creating: data/cora/\n",
            "  inflating: data/cora/cora.cites    \n",
            "  inflating: data/cora/cora.content  \n",
            "  inflating: data/cora/README        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2ed63e88",
      "metadata": {
        "id": "2ed63e88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fa73fc8b",
      "metadata": {
        "id": "fa73fc8b"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "54da9116",
      "metadata": {
        "id": "54da9116"
      },
      "outputs": [],
      "source": [
        "def feature_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c3bc9ac",
      "metadata": {
        "id": "4c3bc9ac"
      },
      "outputs": [],
      "source": [
        "def adj_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1)) # Sum each row\n",
        "    r_inv = np.power(rowsum, -1/2).flatten() # Negative square root\n",
        "#     r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv) # Create diagonal matrix\n",
        "\n",
        "    # D^(-1/2).A.D^(-1/2)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    mx = mx.dot(r_mat_inv)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "73e762d1",
      "metadata": {
        "id": "73e762d1"
      },
      "outputs": [],
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def limit_neighbors(adj_csr, max_neighbors=10, random_seed=42):\n",
        "    \"\"\"\n",
        "    Limit the number of neighbors for each node in the adjacency matrix.\n",
        "\n",
        "    Parameters:\n",
        "    adj_csr (sp.csr_matrix): The original adjacency matrix in CSR format.\n",
        "    max_neighbors (int): The maximum number of neighbors per node.\n",
        "\n",
        "    Returns:\n",
        "    sp.csr_matrix: The modified adjacency matrix with limited neighbors in CSR format.\n",
        "    \"\"\"\n",
        "    adj_coo = adj_csr.tocoo()  # Convert to COO format\n",
        "\n",
        "    if max_neighbors >= adj_coo.shape[0]:\n",
        "        return adj_csr\n",
        "\n",
        "    random_state = np.random.RandomState(random_seed)  # Ensures reproducibility\n",
        "\n",
        "    rows = adj_coo.row\n",
        "    cols = adj_coo.col\n",
        "    data = adj_coo.data\n",
        "\n",
        "    new_rows = []\n",
        "    new_cols = []\n",
        "    new_data = []\n",
        "\n",
        "    for i in range(adj_coo.shape[0]):\n",
        "        mask = rows == i\n",
        "        neighbors = cols[mask]\n",
        "        if len(neighbors) > max_neighbors:\n",
        "            neighbors = random_state.choice(neighbors, max_neighbors, replace=False)\n",
        "        new_rows.extend([i] * len(neighbors))\n",
        "        new_cols.extend(neighbors)\n",
        "        new_data.extend([1] * len(neighbors))  # Assuming unweighted graph\n",
        "\n",
        "    new_adj_coo = sp.coo_matrix((new_data, (new_rows, new_cols)), shape=adj_coo.shape)\n",
        "    return new_adj_coo.tocsr()  # Convert back to CSR format\n"
      ],
      "metadata": {
        "id": "el0NHoKbk6AQ"
      },
      "id": "el0NHoKbk6AQ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ec661dc1",
      "metadata": {
        "id": "ec661dc1"
      },
      "outputs": [],
      "source": [
        "def load_data(path=\"/content/data/cora/\", dataset=\"cora\"):\n",
        "\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) # Processing features into a sparse matrix\n",
        "    labels = encode_onehot(idx_features_labels[:, -1]) # one-hot encoding the labels\n",
        "\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # Reading node-ids\n",
        "    idx_map = {j: i for i, j in enumerate(idx)} # Creating index for nodes to map it in adjacency matrix\n",
        "\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32) # Reading edges\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape) # Mapping node-ids in the edge list to the index\n",
        "\n",
        "    # Build adjacency matrix\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # CHECK OUT THE DIFFERENCES BETWEEN csr_matrix (features) and coo_matrix (adj)\n",
        "\n",
        "    # Normalizing features\n",
        "    features = feature_normalize(features)\n",
        "\n",
        "#     # build symmetric adjacency matrix\n",
        "#     adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    # Normalizing the adjacency matrix after adding self loops\n",
        "    adj_original = adj_normalize(adj + sp.eye(adj.shape[0]))\n",
        "    \"\"\"\n",
        "    To ensure that each of the two hidden layers aggregates from different random samples of neighbors, you need to create two separate modified adjacency matrices,\n",
        "     one for each layer, with different random neighbor selections.\n",
        "Here's how you can adjust the process:\n",
        "\n",
        "Step 1: Modify the Adjacency Matrix Twice with Different Random Seeds\n",
        "    \"\"\"\n",
        "\n",
        "    # Limit the number of neighbors for the first layer\n",
        "    adj_layer1 = limit_neighbors(adj_original, max_neighbors=10, random_seed=42)\n",
        "\n",
        "    # Limit the number of neighbors for the second layer with a different random seed\n",
        "    adj_layer2 = limit_neighbors(adj_original, max_neighbors=10, random_seed=24)\n",
        "\n",
        "    adj_layer1 = sparse_mx_to_torch_sparse_tensor(adj_layer1)\n",
        "    adj_layer2 = sparse_mx_to_torch_sparse_tensor(adj_layer2)\n",
        "\n",
        "\n",
        "\n",
        "    # Setting training, validation, and test range\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    # Converting all matrices into pytorch tensors\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj_layer1, adj_layer2, features, labels, idx_train, idx_val, idx_test  #return them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5bce67b2",
      "metadata": {
        "id": "5bce67b2"
      },
      "outputs": [],
      "source": [
        "# Function to find accuracy from two tensors\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels) # Get the index of maximum value of 1 dimension and typecast to labels datatype\n",
        "    correct = preds.eq(labels).double() # Convert into double\n",
        "    correct = correct.sum() # Sum correct predictions\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "da7e8928",
      "metadata": {
        "id": "da7e8928"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "# Class to define a neural network layer that inherits PyTorch Module\n",
        "# Check out documentaion of the base class 'Module' at:\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "class GraphConvolution(Module):\n",
        "    # Each layer requires no. of input features, no. of output features, and optional bias\n",
        "    def __init__(self, in_feat, out_feat, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "\n",
        "        self.in_features = in_feat\n",
        "        self.out_features = out_feat\n",
        "\n",
        "        # Using Parameter to automatically add weights and bias to learnable parameters\n",
        "        #THIS WILL BE USEFUL ONLY WHEN WE USE Module in the model\n",
        "        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_feat))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # Function to get uniform distribution of weights and bias values\n",
        "    # Can be removed if necessary\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    # Forward function where it actually requires the input data and operations\n",
        "    def forward(self, inp, adj):\n",
        "        # Basically we multiply A.H,W\n",
        "        support = torch.mm(inp, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "\n",
        "        # Adding bias if true\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9b147458",
      "metadata": {
        "id": "9b147458"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\"\"\"\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Class to define the model architecture\n",
        "class GCN(nn.Module):\n",
        "    # The model needs no. of input features, no. of hidden units,\n",
        "    # no. of classes, and optional dropout\n",
        "\n",
        "    # NOTE: We use a simply model with one hidden layer\n",
        "        # Architecture will change for deep models\n",
        "        # Ideally, we keep only a few layers in most GNNs\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # Defining one hidden layer and one output layer\n",
        "        self.gcn1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gcn2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    # Similar to GraphConvolution, we give required input data to the forward function\n",
        "    # And specify operations - here it is activation and dropout\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gcn1(x, adj)) # Applying non-linearity on hidden layer 1\n",
        "        # Checkout difference between nn.Dropout() and F.dropout()\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gcn2(x, adj)\n",
        "        return F.log_softmax(x, dim=1) # Applying lograthmic softmax on output layer\n",
        "\n",
        "\"\"\"\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nhid2, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gcn1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gcn2 = GraphConvolution(nhid, nhid2)\n",
        "        self.gcn3 = GraphConvolution(nhid2, nclass)\n",
        "        self.dropout = dropout\n",
        "#####Step 2: Modify the GCN Forward Pass\n",
        "#########Adjust the forward pass of your GCN model to accept two adjacency matrices and use them for the different layers.\n",
        "    def forward(self, x, adj_layer1, adj_layer2):\n",
        "        x = F.relu(self.gcn1(x, adj_layer1))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.relu(self.gcn2(x, adj_layer2))\n",
        "        x = self.gcn3(x, adj_layer2)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "990494b4",
      "metadata": {
        "id": "990494b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8070a089-9981-4478-b189-ed0fcbd79987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-3650411315.py:8: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:644.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "adj_layer1, adj_layer2, features, labels, train_ids, val_ids, test_ids = load_data()\n",
        "\"\"\"\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "\"\"\"\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nhid2=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "\n",
        "\n",
        "# Using Adam optimizer. Other optimizer can be used too\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=0.01, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e2542c4d",
      "metadata": {
        "id": "e2542c4d"
      },
      "outputs": [],
      "source": [
        "# Code for GPU computing\n",
        "\n",
        "# CHANGE THIS CODE TO SUIT YOUR VERSION OF PYTORCH. THE SYNTAX OF THIS COULD VARY SIGNIFICANTLY\n",
        "\n",
        "# If cuda is available, move all data to gpu\n",
        "# And preparing for CUDA operations\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj_layer1 = adj_layer1.cuda()\n",
        "    adj_layer2 = adj_layer2.cuda()\n",
        "    labels = labels.cuda()\n",
        "    train_ids.cuda()\n",
        "    val_ids.cuda()\n",
        "    test_ids.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "146eb197",
      "metadata": {
        "id": "146eb197"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "\n",
        "    # Evaluating the model in training mode\n",
        "    model.train()\n",
        "    optimizer.zero_grad() # Reseting gradient at each layer to avoid exploding gradient problem\n",
        "    output = model(features, adj_layer1, adj_layer2)\n",
        "     # Optimizing with nll_loss. Other losses like cross_entropy can also be used\n",
        "    loss_train = F.nll_loss(output[train_ids], labels[train_ids])\n",
        "    acc_train = accuracy(output[train_ids], labels[train_ids])\n",
        "    # backprop and optimize model parameters\n",
        "    # Not needed to specify parameters when using Parameter\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluating validation performance separately\n",
        "    model.eval()\n",
        "    output = model(features, adj_layer1, adj_layer2)\n",
        "    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n",
        "    acc_val = accuracy(output[val_ids], labels[val_ids])\n",
        "\n",
        "\n",
        "    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n",
        "    acc_val = accuracy(output[val_ids], labels[val_ids])\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b32be098",
      "metadata": {
        "id": "b32be098"
      },
      "outputs": [],
      "source": [
        "# Function to test the model\n",
        "\"\"\"\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[test_ids], labels[test_ids])\n",
        "    acc_test = accuracy(output[test_ids], labels[test_ids])\n",
        "\n",
        "\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(features, adj_layer1, adj_layer2)\n",
        "        loss_test = F.nll_loss(output[test_ids], labels[test_ids])\n",
        "        acc_test = accuracy(output[test_ids], labels[test_ids])\n",
        "\n",
        "        _, predicted = torch.max(output[test_ids], 1)\n",
        "\n",
        "        # Moving the data to CPU for sklearn metrics calculation\n",
        "        predicted_np = predicted.cpu().numpy()\n",
        "        labels_np = labels[test_ids].cpu().numpy()\n",
        "\n",
        "        precision = precision_score(labels_np, predicted_np, average='weighted')\n",
        "        recall = recall_score(labels_np, predicted_np, average='weighted')\n",
        "        f1 = f1_score(labels_np, predicted_np, average='weighted')\n",
        "\n",
        "        print(f\"Test set results: loss= {loss_test.item():.4f}, accuracy= {acc_test:.4f}, precision= {precision:.4f}, recall= {recall:.4f}, F1 Score= {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a32da91b",
      "metadata": {
        "id": "a32da91b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b7d0bb-e9f1-4683-f050-77aee984eec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9729 acc_train: 0.2071 loss_val: 1.9338 acc_val: 0.2000 time: 0.1428s\n",
            "Epoch: 0002 loss_train: 1.8903 acc_train: 0.1714 loss_val: 1.9384 acc_val: 0.1633 time: 0.0281s\n",
            "Epoch: 0003 loss_train: 1.8436 acc_train: 0.2071 loss_val: 1.9312 acc_val: 0.1767 time: 0.0290s\n",
            "Epoch: 0004 loss_train: 1.8394 acc_train: 0.2286 loss_val: 1.9031 acc_val: 0.2133 time: 0.0316s\n",
            "Epoch: 0005 loss_train: 1.7863 acc_train: 0.2429 loss_val: 1.8754 acc_val: 0.2667 time: 0.0295s\n",
            "Epoch: 0006 loss_train: 1.7326 acc_train: 0.3071 loss_val: 1.8526 acc_val: 0.3067 time: 0.0294s\n",
            "Epoch: 0007 loss_train: 1.7058 acc_train: 0.3571 loss_val: 1.8296 acc_val: 0.3100 time: 0.0321s\n",
            "Epoch: 0008 loss_train: 1.6744 acc_train: 0.3429 loss_val: 1.8022 acc_val: 0.3633 time: 0.0313s\n",
            "Epoch: 0009 loss_train: 1.6659 acc_train: 0.3929 loss_val: 1.7724 acc_val: 0.4233 time: 0.0287s\n",
            "Epoch: 0010 loss_train: 1.6056 acc_train: 0.4429 loss_val: 1.7432 acc_val: 0.4467 time: 0.0278s\n",
            "Epoch: 0011 loss_train: 1.5701 acc_train: 0.4500 loss_val: 1.7138 acc_val: 0.4967 time: 0.0282s\n",
            "Epoch: 0012 loss_train: 1.5218 acc_train: 0.4571 loss_val: 1.6850 acc_val: 0.5000 time: 0.0270s\n",
            "Epoch: 0013 loss_train: 1.4786 acc_train: 0.4643 loss_val: 1.6564 acc_val: 0.5100 time: 0.0279s\n",
            "Epoch: 0014 loss_train: 1.4273 acc_train: 0.5143 loss_val: 1.6274 acc_val: 0.4967 time: 0.0272s\n",
            "Epoch: 0015 loss_train: 1.4124 acc_train: 0.5071 loss_val: 1.5981 acc_val: 0.5000 time: 0.0293s\n",
            "Epoch: 0016 loss_train: 1.3695 acc_train: 0.5214 loss_val: 1.5680 acc_val: 0.5267 time: 0.0295s\n",
            "Epoch: 0017 loss_train: 1.3326 acc_train: 0.5571 loss_val: 1.5413 acc_val: 0.5500 time: 0.0276s\n",
            "Epoch: 0018 loss_train: 1.2935 acc_train: 0.5571 loss_val: 1.5183 acc_val: 0.5500 time: 0.0271s\n",
            "Epoch: 0019 loss_train: 1.2455 acc_train: 0.6000 loss_val: 1.4983 acc_val: 0.5467 time: 0.0281s\n",
            "Epoch: 0020 loss_train: 1.2215 acc_train: 0.5929 loss_val: 1.4786 acc_val: 0.5500 time: 0.0280s\n",
            "Epoch: 0021 loss_train: 1.1919 acc_train: 0.5929 loss_val: 1.4633 acc_val: 0.5600 time: 0.0284s\n",
            "Epoch: 0022 loss_train: 1.1338 acc_train: 0.6786 loss_val: 1.4477 acc_val: 0.5600 time: 0.0282s\n",
            "Epoch: 0023 loss_train: 1.1508 acc_train: 0.6214 loss_val: 1.4261 acc_val: 0.5633 time: 0.0280s\n",
            "Epoch: 0024 loss_train: 1.1012 acc_train: 0.6357 loss_val: 1.4026 acc_val: 0.5667 time: 0.0354s\n",
            "Epoch: 0025 loss_train: 1.0703 acc_train: 0.6929 loss_val: 1.3815 acc_val: 0.5733 time: 0.0296s\n",
            "Epoch: 0026 loss_train: 1.0533 acc_train: 0.6357 loss_val: 1.3711 acc_val: 0.5800 time: 0.0307s\n",
            "Epoch: 0027 loss_train: 0.9956 acc_train: 0.6929 loss_val: 1.3667 acc_val: 0.5933 time: 0.0337s\n",
            "Epoch: 0028 loss_train: 0.9712 acc_train: 0.6929 loss_val: 1.3614 acc_val: 0.5967 time: 0.0311s\n",
            "Epoch: 0029 loss_train: 0.9598 acc_train: 0.6929 loss_val: 1.3544 acc_val: 0.6100 time: 0.0292s\n",
            "Epoch: 0030 loss_train: 0.9206 acc_train: 0.7143 loss_val: 1.3483 acc_val: 0.6133 time: 0.0350s\n",
            "Epoch: 0031 loss_train: 0.9041 acc_train: 0.7000 loss_val: 1.3478 acc_val: 0.6133 time: 0.0389s\n",
            "Epoch: 0032 loss_train: 0.8804 acc_train: 0.6929 loss_val: 1.3360 acc_val: 0.6233 time: 0.0279s\n",
            "Epoch: 0033 loss_train: 0.8643 acc_train: 0.7143 loss_val: 1.3212 acc_val: 0.6267 time: 0.0273s\n",
            "Epoch: 0034 loss_train: 0.8223 acc_train: 0.7286 loss_val: 1.3078 acc_val: 0.6333 time: 0.0289s\n",
            "Epoch: 0035 loss_train: 0.8258 acc_train: 0.7357 loss_val: 1.2942 acc_val: 0.6367 time: 0.0293s\n",
            "Epoch: 0036 loss_train: 0.8275 acc_train: 0.7500 loss_val: 1.3315 acc_val: 0.6367 time: 0.0289s\n",
            "Epoch: 0037 loss_train: 0.7393 acc_train: 0.7714 loss_val: 1.3815 acc_val: 0.6267 time: 0.0288s\n",
            "Epoch: 0038 loss_train: 0.7854 acc_train: 0.7571 loss_val: 1.4012 acc_val: 0.6333 time: 0.0297s\n",
            "Epoch: 0039 loss_train: 0.7600 acc_train: 0.7429 loss_val: 1.3960 acc_val: 0.6367 time: 0.0311s\n",
            "Epoch: 0040 loss_train: 0.6935 acc_train: 0.8143 loss_val: 1.3937 acc_val: 0.6500 time: 0.0283s\n",
            "Epoch: 0041 loss_train: 0.7052 acc_train: 0.8000 loss_val: 1.3992 acc_val: 0.6433 time: 0.0286s\n",
            "Epoch: 0042 loss_train: 0.6620 acc_train: 0.8071 loss_val: 1.3959 acc_val: 0.6333 time: 0.0286s\n",
            "Epoch: 0043 loss_train: 0.6607 acc_train: 0.8429 loss_val: 1.3746 acc_val: 0.6367 time: 0.0275s\n",
            "Epoch: 0044 loss_train: 0.6797 acc_train: 0.8071 loss_val: 1.3472 acc_val: 0.6567 time: 0.0295s\n",
            "Epoch: 0045 loss_train: 0.6661 acc_train: 0.8071 loss_val: 1.3237 acc_val: 0.6567 time: 0.0328s\n",
            "Epoch: 0046 loss_train: 0.6400 acc_train: 0.7929 loss_val: 1.3113 acc_val: 0.6600 time: 0.0280s\n",
            "Epoch: 0047 loss_train: 0.6769 acc_train: 0.7571 loss_val: 1.3091 acc_val: 0.6600 time: 0.0282s\n",
            "Epoch: 0048 loss_train: 0.5477 acc_train: 0.8500 loss_val: 1.3222 acc_val: 0.6700 time: 0.0305s\n",
            "Epoch: 0049 loss_train: 0.5928 acc_train: 0.8000 loss_val: 1.3203 acc_val: 0.6633 time: 0.0293s\n",
            "Epoch: 0050 loss_train: 0.5406 acc_train: 0.8286 loss_val: 1.3384 acc_val: 0.6700 time: 0.0277s\n",
            "Epoch: 0051 loss_train: 0.5343 acc_train: 0.8143 loss_val: 1.3668 acc_val: 0.6833 time: 0.0290s\n",
            "Epoch: 0052 loss_train: 0.5844 acc_train: 0.8071 loss_val: 1.4022 acc_val: 0.6933 time: 0.0320s\n",
            "Epoch: 0053 loss_train: 0.5835 acc_train: 0.8357 loss_val: 1.4139 acc_val: 0.6933 time: 0.0276s\n",
            "Epoch: 0054 loss_train: 0.5172 acc_train: 0.8643 loss_val: 1.4203 acc_val: 0.6900 time: 0.0273s\n",
            "Epoch: 0055 loss_train: 0.5027 acc_train: 0.8714 loss_val: 1.4275 acc_val: 0.6933 time: 0.0274s\n",
            "Epoch: 0056 loss_train: 0.5002 acc_train: 0.8571 loss_val: 1.4402 acc_val: 0.6833 time: 0.0275s\n",
            "Epoch: 0057 loss_train: 0.4483 acc_train: 0.8786 loss_val: 1.4486 acc_val: 0.6900 time: 0.0331s\n",
            "Epoch: 0058 loss_train: 0.5047 acc_train: 0.8643 loss_val: 1.4587 acc_val: 0.6767 time: 0.0306s\n",
            "Epoch: 0059 loss_train: 0.4451 acc_train: 0.8857 loss_val: 1.4525 acc_val: 0.6933 time: 0.0306s\n",
            "Epoch: 0060 loss_train: 0.4708 acc_train: 0.8500 loss_val: 1.4530 acc_val: 0.6967 time: 0.0281s\n",
            "Epoch: 0061 loss_train: 0.4897 acc_train: 0.8786 loss_val: 1.4949 acc_val: 0.6967 time: 0.0275s\n",
            "Epoch: 0062 loss_train: 0.4717 acc_train: 0.8643 loss_val: 1.5436 acc_val: 0.6933 time: 0.0276s\n",
            "Epoch: 0063 loss_train: 0.4748 acc_train: 0.8429 loss_val: 1.5885 acc_val: 0.6933 time: 0.0281s\n",
            "Epoch: 0064 loss_train: 0.4311 acc_train: 0.8929 loss_val: 1.6280 acc_val: 0.6867 time: 0.0284s\n",
            "Epoch: 0065 loss_train: 0.4313 acc_train: 0.8857 loss_val: 1.6481 acc_val: 0.6900 time: 0.0296s\n",
            "Epoch: 0066 loss_train: 0.4212 acc_train: 0.8429 loss_val: 1.5784 acc_val: 0.6967 time: 0.0394s\n",
            "Epoch: 0067 loss_train: 0.4198 acc_train: 0.8857 loss_val: 1.4722 acc_val: 0.6833 time: 0.0312s\n",
            "Epoch: 0068 loss_train: 0.4144 acc_train: 0.9000 loss_val: 1.4322 acc_val: 0.6867 time: 0.0287s\n",
            "Epoch: 0069 loss_train: 0.4204 acc_train: 0.8786 loss_val: 1.4221 acc_val: 0.6900 time: 0.0273s\n",
            "Epoch: 0070 loss_train: 0.3864 acc_train: 0.8857 loss_val: 1.4349 acc_val: 0.6600 time: 0.0275s\n",
            "Epoch: 0071 loss_train: 0.4278 acc_train: 0.8500 loss_val: 1.4467 acc_val: 0.6633 time: 0.0282s\n",
            "Epoch: 0072 loss_train: 0.3908 acc_train: 0.9286 loss_val: 1.4633 acc_val: 0.6800 time: 0.0275s\n",
            "Epoch: 0073 loss_train: 0.3547 acc_train: 0.9143 loss_val: 1.4872 acc_val: 0.6800 time: 0.0348s\n",
            "Epoch: 0074 loss_train: 0.3480 acc_train: 0.9571 loss_val: 1.5279 acc_val: 0.6800 time: 0.0283s\n",
            "Epoch: 0075 loss_train: 0.4346 acc_train: 0.8571 loss_val: 1.5905 acc_val: 0.6800 time: 0.0293s\n",
            "Epoch: 0076 loss_train: 0.3594 acc_train: 0.9143 loss_val: 1.6773 acc_val: 0.6833 time: 0.0281s\n",
            "Epoch: 0077 loss_train: 0.3981 acc_train: 0.9143 loss_val: 1.7765 acc_val: 0.6767 time: 0.0281s\n",
            "Epoch: 0078 loss_train: 0.3537 acc_train: 0.9071 loss_val: 1.8720 acc_val: 0.6800 time: 0.0394s\n",
            "Epoch: 0079 loss_train: 0.3182 acc_train: 0.9357 loss_val: 1.9535 acc_val: 0.6600 time: 0.0295s\n",
            "Epoch: 0080 loss_train: 0.4082 acc_train: 0.8786 loss_val: 2.0022 acc_val: 0.6500 time: 0.0302s\n",
            "Epoch: 0081 loss_train: 0.3401 acc_train: 0.9143 loss_val: 1.9468 acc_val: 0.6433 time: 0.0293s\n",
            "Epoch: 0082 loss_train: 0.3392 acc_train: 0.9071 loss_val: 1.8773 acc_val: 0.6533 time: 0.0290s\n",
            "Epoch: 0083 loss_train: 0.3518 acc_train: 0.9143 loss_val: 1.8188 acc_val: 0.6667 time: 0.0277s\n",
            "Epoch: 0084 loss_train: 0.2909 acc_train: 0.9214 loss_val: 1.7815 acc_val: 0.6600 time: 0.0280s\n",
            "Epoch: 0085 loss_train: 0.3411 acc_train: 0.9071 loss_val: 1.7519 acc_val: 0.6600 time: 0.0277s\n",
            "Epoch: 0086 loss_train: 0.3383 acc_train: 0.8786 loss_val: 1.7513 acc_val: 0.6633 time: 0.0334s\n",
            "Epoch: 0087 loss_train: 0.3195 acc_train: 0.9143 loss_val: 1.7738 acc_val: 0.6700 time: 0.0310s\n",
            "Epoch: 0088 loss_train: 0.2868 acc_train: 0.9357 loss_val: 1.8139 acc_val: 0.6700 time: 0.0310s\n",
            "Epoch: 0089 loss_train: 0.3617 acc_train: 0.8714 loss_val: 1.8667 acc_val: 0.6700 time: 0.0292s\n",
            "Epoch: 0090 loss_train: 0.3749 acc_train: 0.8643 loss_val: 1.8904 acc_val: 0.6733 time: 0.0288s\n",
            "Epoch: 0091 loss_train: 0.3487 acc_train: 0.9000 loss_val: 1.8516 acc_val: 0.6633 time: 0.0287s\n",
            "Epoch: 0092 loss_train: 0.3659 acc_train: 0.8857 loss_val: 1.8261 acc_val: 0.6667 time: 0.0284s\n",
            "Epoch: 0093 loss_train: 0.3011 acc_train: 0.9214 loss_val: 1.8272 acc_val: 0.6500 time: 0.0308s\n",
            "Epoch: 0094 loss_train: 0.2825 acc_train: 0.9143 loss_val: 1.8347 acc_val: 0.6433 time: 0.0342s\n",
            "Epoch: 0095 loss_train: 0.3257 acc_train: 0.9000 loss_val: 1.8170 acc_val: 0.6667 time: 0.0310s\n",
            "Epoch: 0096 loss_train: 0.2942 acc_train: 0.9071 loss_val: 1.8026 acc_val: 0.6667 time: 0.0287s\n",
            "Epoch: 0097 loss_train: 0.3317 acc_train: 0.8929 loss_val: 1.8062 acc_val: 0.6700 time: 0.0299s\n",
            "Epoch: 0098 loss_train: 0.2797 acc_train: 0.9071 loss_val: 1.8156 acc_val: 0.6700 time: 0.0286s\n",
            "Epoch: 0099 loss_train: 0.3159 acc_train: 0.8857 loss_val: 1.8237 acc_val: 0.6633 time: 0.0305s\n",
            "Epoch: 0100 loss_train: 0.3029 acc_train: 0.9286 loss_val: 1.8467 acc_val: 0.6667 time: 0.0316s\n",
            "Epoch: 0101 loss_train: 0.2803 acc_train: 0.9214 loss_val: 1.8813 acc_val: 0.6700 time: 0.0391s\n",
            "Epoch: 0102 loss_train: 0.2362 acc_train: 0.9357 loss_val: 1.9345 acc_val: 0.6733 time: 0.0275s\n",
            "Epoch: 0103 loss_train: 0.2584 acc_train: 0.9286 loss_val: 1.9851 acc_val: 0.6800 time: 0.0315s\n",
            "Epoch: 0104 loss_train: 0.2727 acc_train: 0.9214 loss_val: 1.9988 acc_val: 0.6833 time: 0.0278s\n",
            "Epoch: 0105 loss_train: 0.2473 acc_train: 0.9286 loss_val: 2.0040 acc_val: 0.6867 time: 0.0274s\n",
            "Epoch: 0106 loss_train: 0.2712 acc_train: 0.9214 loss_val: 1.9516 acc_val: 0.6767 time: 0.0301s\n",
            "Epoch: 0107 loss_train: 0.2689 acc_train: 0.9357 loss_val: 1.9030 acc_val: 0.6667 time: 0.0286s\n",
            "Epoch: 0108 loss_train: 0.2796 acc_train: 0.8857 loss_val: 1.9002 acc_val: 0.6700 time: 0.0307s\n",
            "Epoch: 0109 loss_train: 0.2409 acc_train: 0.9286 loss_val: 1.9103 acc_val: 0.6667 time: 0.0288s\n",
            "Epoch: 0110 loss_train: 0.3309 acc_train: 0.8786 loss_val: 1.9058 acc_val: 0.6667 time: 0.0289s\n",
            "Epoch: 0111 loss_train: 0.2819 acc_train: 0.9143 loss_val: 1.9008 acc_val: 0.6600 time: 0.0315s\n",
            "Epoch: 0112 loss_train: 0.2734 acc_train: 0.9143 loss_val: 1.8892 acc_val: 0.6667 time: 0.0303s\n",
            "Epoch: 0113 loss_train: 0.2475 acc_train: 0.9000 loss_val: 1.8600 acc_val: 0.6667 time: 0.0289s\n",
            "Epoch: 0114 loss_train: 0.2282 acc_train: 0.9429 loss_val: 1.8553 acc_val: 0.6700 time: 0.0288s\n",
            "Epoch: 0115 loss_train: 0.2974 acc_train: 0.9000 loss_val: 1.8675 acc_val: 0.6833 time: 0.0331s\n",
            "Epoch: 0116 loss_train: 0.3084 acc_train: 0.8929 loss_val: 1.8869 acc_val: 0.6833 time: 0.0281s\n",
            "Epoch: 0117 loss_train: 0.2578 acc_train: 0.9143 loss_val: 1.9061 acc_val: 0.6933 time: 0.0289s\n",
            "Epoch: 0118 loss_train: 0.1880 acc_train: 0.9500 loss_val: 1.9223 acc_val: 0.6933 time: 0.0288s\n",
            "Epoch: 0119 loss_train: 0.2683 acc_train: 0.9357 loss_val: 1.9266 acc_val: 0.6800 time: 0.0303s\n",
            "Epoch: 0120 loss_train: 0.1915 acc_train: 0.9571 loss_val: 2.0061 acc_val: 0.6633 time: 0.0293s\n",
            "Epoch: 0121 loss_train: 0.2591 acc_train: 0.9214 loss_val: 2.1344 acc_val: 0.6567 time: 0.0279s\n",
            "Epoch: 0122 loss_train: 0.2844 acc_train: 0.9000 loss_val: 2.2913 acc_val: 0.6433 time: 0.0308s\n",
            "Epoch: 0123 loss_train: 0.2810 acc_train: 0.9071 loss_val: 2.3807 acc_val: 0.6333 time: 0.0370s\n",
            "Epoch: 0124 loss_train: 0.2171 acc_train: 0.9286 loss_val: 2.2733 acc_val: 0.6467 time: 0.0308s\n",
            "Epoch: 0125 loss_train: 0.2699 acc_train: 0.8857 loss_val: 2.1400 acc_val: 0.6333 time: 0.0297s\n",
            "Epoch: 0126 loss_train: 0.2600 acc_train: 0.9143 loss_val: 2.0525 acc_val: 0.6467 time: 0.0284s\n",
            "Epoch: 0127 loss_train: 0.1994 acc_train: 0.9571 loss_val: 1.9943 acc_val: 0.6633 time: 0.0279s\n",
            "Epoch: 0128 loss_train: 0.2491 acc_train: 0.9214 loss_val: 1.9653 acc_val: 0.6800 time: 0.0283s\n",
            "Epoch: 0129 loss_train: 0.2618 acc_train: 0.9143 loss_val: 1.9656 acc_val: 0.6833 time: 0.0314s\n",
            "Epoch: 0130 loss_train: 0.2128 acc_train: 0.9500 loss_val: 1.9754 acc_val: 0.6833 time: 0.0282s\n",
            "Epoch: 0131 loss_train: 0.2503 acc_train: 0.9286 loss_val: 1.9870 acc_val: 0.6800 time: 0.0287s\n",
            "Epoch: 0132 loss_train: 0.2950 acc_train: 0.9000 loss_val: 1.9938 acc_val: 0.6833 time: 0.0283s\n",
            "Epoch: 0133 loss_train: 0.2442 acc_train: 0.9214 loss_val: 2.0446 acc_val: 0.6867 time: 0.0286s\n",
            "Epoch: 0134 loss_train: 0.2102 acc_train: 0.9286 loss_val: 2.1273 acc_val: 0.6800 time: 0.0282s\n",
            "Epoch: 0135 loss_train: 0.2043 acc_train: 0.9286 loss_val: 2.1965 acc_val: 0.6633 time: 0.0316s\n",
            "Epoch: 0136 loss_train: 0.2344 acc_train: 0.9357 loss_val: 2.2212 acc_val: 0.6533 time: 0.0410s\n",
            "Epoch: 0137 loss_train: 0.2027 acc_train: 0.9357 loss_val: 2.2376 acc_val: 0.6500 time: 0.0272s\n",
            "Epoch: 0138 loss_train: 0.2172 acc_train: 0.9286 loss_val: 2.2553 acc_val: 0.6533 time: 0.0280s\n",
            "Epoch: 0139 loss_train: 0.2207 acc_train: 0.9357 loss_val: 2.2582 acc_val: 0.6500 time: 0.0287s\n",
            "Epoch: 0140 loss_train: 0.2041 acc_train: 0.9643 loss_val: 2.2538 acc_val: 0.6333 time: 0.0315s\n",
            "Epoch: 0141 loss_train: 0.2238 acc_train: 0.9286 loss_val: 2.2426 acc_val: 0.6400 time: 0.0291s\n",
            "Epoch: 0142 loss_train: 0.2679 acc_train: 0.9000 loss_val: 2.2220 acc_val: 0.6467 time: 0.0281s\n",
            "Epoch: 0143 loss_train: 0.2252 acc_train: 0.9286 loss_val: 2.2034 acc_val: 0.6467 time: 0.0318s\n",
            "Epoch: 0144 loss_train: 0.2493 acc_train: 0.9357 loss_val: 2.2140 acc_val: 0.6500 time: 0.0310s\n",
            "Epoch: 0145 loss_train: 0.1975 acc_train: 0.9286 loss_val: 2.2136 acc_val: 0.6567 time: 0.0296s\n",
            "Epoch: 0146 loss_train: 0.2314 acc_train: 0.9071 loss_val: 2.2063 acc_val: 0.6600 time: 0.0286s\n",
            "Epoch: 0147 loss_train: 0.2542 acc_train: 0.8929 loss_val: 2.1967 acc_val: 0.6633 time: 0.0303s\n",
            "Epoch: 0148 loss_train: 0.2187 acc_train: 0.9357 loss_val: 2.1846 acc_val: 0.6700 time: 0.0290s\n",
            "Epoch: 0149 loss_train: 0.2055 acc_train: 0.9571 loss_val: 2.1717 acc_val: 0.6733 time: 0.0308s\n",
            "Epoch: 0150 loss_train: 0.1953 acc_train: 0.9643 loss_val: 2.1539 acc_val: 0.6733 time: 0.0318s\n",
            "Epoch: 0151 loss_train: 0.1828 acc_train: 0.9500 loss_val: 2.1467 acc_val: 0.6733 time: 0.0287s\n",
            "Epoch: 0152 loss_train: 0.2151 acc_train: 0.9357 loss_val: 2.1363 acc_val: 0.6667 time: 0.0296s\n",
            "Epoch: 0153 loss_train: 0.2109 acc_train: 0.9429 loss_val: 2.1313 acc_val: 0.6700 time: 0.0298s\n",
            "Epoch: 0154 loss_train: 0.1869 acc_train: 0.9143 loss_val: 2.1386 acc_val: 0.6767 time: 0.0291s\n",
            "Epoch: 0155 loss_train: 0.2023 acc_train: 0.9357 loss_val: 2.1542 acc_val: 0.6767 time: 0.0298s\n",
            "Epoch: 0156 loss_train: 0.1906 acc_train: 0.9286 loss_val: 2.1730 acc_val: 0.6667 time: 0.0289s\n",
            "Epoch: 0157 loss_train: 0.1699 acc_train: 0.9500 loss_val: 2.1913 acc_val: 0.6633 time: 0.0311s\n",
            "Epoch: 0158 loss_train: 0.2116 acc_train: 0.9429 loss_val: 2.2006 acc_val: 0.6600 time: 0.0288s\n",
            "Epoch: 0159 loss_train: 0.1846 acc_train: 0.9571 loss_val: 2.2105 acc_val: 0.6567 time: 0.0268s\n",
            "Epoch: 0160 loss_train: 0.1708 acc_train: 0.9571 loss_val: 2.2191 acc_val: 0.6533 time: 0.0271s\n",
            "Epoch: 0161 loss_train: 0.1411 acc_train: 0.9786 loss_val: 2.2249 acc_val: 0.6567 time: 0.0280s\n",
            "Epoch: 0162 loss_train: 0.1549 acc_train: 0.9714 loss_val: 2.2325 acc_val: 0.6567 time: 0.0281s\n",
            "Epoch: 0163 loss_train: 0.1925 acc_train: 0.9357 loss_val: 2.2418 acc_val: 0.6533 time: 0.0284s\n",
            "Epoch: 0164 loss_train: 0.1575 acc_train: 0.9786 loss_val: 2.2572 acc_val: 0.6500 time: 0.0286s\n",
            "Epoch: 0165 loss_train: 0.2137 acc_train: 0.9286 loss_val: 2.2610 acc_val: 0.6533 time: 0.0293s\n",
            "Epoch: 0166 loss_train: 0.1586 acc_train: 0.9643 loss_val: 2.2709 acc_val: 0.6467 time: 0.0280s\n",
            "Epoch: 0167 loss_train: 0.1672 acc_train: 0.9500 loss_val: 2.2800 acc_val: 0.6467 time: 0.0278s\n",
            "Epoch: 0168 loss_train: 0.1226 acc_train: 0.9714 loss_val: 2.2887 acc_val: 0.6467 time: 0.0280s\n",
            "Epoch: 0169 loss_train: 0.1727 acc_train: 0.9500 loss_val: 2.2940 acc_val: 0.6400 time: 0.0284s\n",
            "Epoch: 0170 loss_train: 0.2489 acc_train: 0.9286 loss_val: 2.2432 acc_val: 0.6500 time: 0.0287s\n",
            "Epoch: 0171 loss_train: 0.1631 acc_train: 0.9571 loss_val: 2.2321 acc_val: 0.6600 time: 0.0378s\n",
            "Epoch: 0172 loss_train: 0.2094 acc_train: 0.9286 loss_val: 2.2385 acc_val: 0.6533 time: 0.0330s\n",
            "Epoch: 0173 loss_train: 0.2969 acc_train: 0.9143 loss_val: 2.1862 acc_val: 0.6733 time: 0.0317s\n",
            "Epoch: 0174 loss_train: 0.1521 acc_train: 0.9500 loss_val: 2.2064 acc_val: 0.6633 time: 0.0270s\n",
            "Epoch: 0175 loss_train: 0.1628 acc_train: 0.9571 loss_val: 2.2600 acc_val: 0.6633 time: 0.0289s\n",
            "Epoch: 0176 loss_train: 0.1868 acc_train: 0.9429 loss_val: 2.3268 acc_val: 0.6733 time: 0.0275s\n",
            "Epoch: 0177 loss_train: 0.2782 acc_train: 0.9214 loss_val: 2.3113 acc_val: 0.6533 time: 0.0279s\n",
            "Epoch: 0178 loss_train: 0.1928 acc_train: 0.9357 loss_val: 2.3131 acc_val: 0.6400 time: 0.0292s\n",
            "Epoch: 0179 loss_train: 0.1916 acc_train: 0.9500 loss_val: 2.2998 acc_val: 0.6367 time: 0.0308s\n",
            "Epoch: 0180 loss_train: 0.1689 acc_train: 0.9500 loss_val: 2.3105 acc_val: 0.6433 time: 0.0313s\n",
            "Epoch: 0181 loss_train: 0.1805 acc_train: 0.9429 loss_val: 2.3261 acc_val: 0.6500 time: 0.0287s\n",
            "Epoch: 0182 loss_train: 0.1589 acc_train: 0.9571 loss_val: 2.3478 acc_val: 0.6433 time: 0.0294s\n",
            "Epoch: 0183 loss_train: 0.2672 acc_train: 0.9071 loss_val: 2.3180 acc_val: 0.6400 time: 0.0296s\n",
            "Epoch: 0184 loss_train: 0.1438 acc_train: 0.9714 loss_val: 2.3071 acc_val: 0.6333 time: 0.0282s\n",
            "Epoch: 0185 loss_train: 0.1318 acc_train: 0.9643 loss_val: 2.2945 acc_val: 0.6300 time: 0.0270s\n",
            "Epoch: 0186 loss_train: 0.1872 acc_train: 0.9429 loss_val: 2.2846 acc_val: 0.6333 time: 0.0298s\n",
            "Epoch: 0187 loss_train: 0.2020 acc_train: 0.9429 loss_val: 2.2712 acc_val: 0.6400 time: 0.0290s\n",
            "Epoch: 0188 loss_train: 0.1499 acc_train: 0.9643 loss_val: 2.2611 acc_val: 0.6400 time: 0.0281s\n",
            "Epoch: 0189 loss_train: 0.2560 acc_train: 0.9357 loss_val: 2.2581 acc_val: 0.6367 time: 0.0283s\n",
            "Epoch: 0190 loss_train: 0.1884 acc_train: 0.9357 loss_val: 2.2610 acc_val: 0.6333 time: 0.0285s\n",
            "Epoch: 0191 loss_train: 0.1638 acc_train: 0.9500 loss_val: 2.2745 acc_val: 0.6367 time: 0.0286s\n",
            "Epoch: 0192 loss_train: 0.1915 acc_train: 0.9214 loss_val: 2.2883 acc_val: 0.6300 time: 0.0292s\n",
            "Epoch: 0193 loss_train: 0.1827 acc_train: 0.9429 loss_val: 2.2895 acc_val: 0.6300 time: 0.0313s\n",
            "Epoch: 0194 loss_train: 0.1816 acc_train: 0.9429 loss_val: 2.2860 acc_val: 0.6400 time: 0.0288s\n",
            "Epoch: 0195 loss_train: 0.1941 acc_train: 0.9429 loss_val: 2.2784 acc_val: 0.6467 time: 0.0284s\n",
            "Epoch: 0196 loss_train: 0.2082 acc_train: 0.9500 loss_val: 2.2789 acc_val: 0.6567 time: 0.0284s\n",
            "Epoch: 0197 loss_train: 0.1487 acc_train: 0.9643 loss_val: 2.2768 acc_val: 0.6567 time: 0.0287s\n",
            "Epoch: 0198 loss_train: 0.1644 acc_train: 0.9429 loss_val: 2.2747 acc_val: 0.6500 time: 0.0285s\n",
            "Epoch: 0199 loss_train: 0.1399 acc_train: 0.9714 loss_val: 2.2670 acc_val: 0.6467 time: 0.0304s\n",
            "Epoch: 0200 loss_train: 0.1542 acc_train: 0.9643 loss_val: 2.2691 acc_val: 0.6533 time: 0.0296s\n",
            "\n",
            "Optimization finished...\n",
            "Total time elapsed: 6.0780s\n",
            "Test set results: loss= 2.1325, accuracy= 0.6140, precision= 0.6286, recall= 0.6140, F1 Score= 0.6134\n"
          ]
        }
      ],
      "source": [
        "t_total = time.time()\n",
        "\n",
        "for epoch in range(200):\n",
        "    train(epoch)\n",
        "\n",
        "print()\n",
        "print('Optimization finished...')\n",
        "print('Total time elapsed: {:.4f}s'.format(time.time() - t_total))\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af85139",
      "metadata": {
        "id": "1af85139"
      },
      "source": [
        "###"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}