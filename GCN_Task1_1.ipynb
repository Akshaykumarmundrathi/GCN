{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshaykumarmundrathi/GCN/blob/main/GCN_Task1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying the GCN Model for 2 Hidden Layers\n",
        "To incorporate an additional hidden layer, create a new instance of GraphConvolution in the GCN class and update the forward method.\n",
        "\n",
        "Define an Additional Layer:\n",
        "\n",
        "Add a new GraphConvolution layer in the GCN class, let's call it gcn2.\n",
        "The existing gcn2 will be renamed to gcn3 as it will now act as the output layer.\n",
        "\n",
        "Modify the Constructor:\n",
        "\n",
        "Update the __init__ method in the GCN class to include the new layer gcn2.\n",
        "Decide the number of hidden units for the new layer (nhid2). The input features for gcn2 will be nhid, and the output features can be nhid2.\n",
        "\n",
        "Adjust the Forward Method:\n",
        "\n",
        "Modify the forward method to include the new layer with an activation function (like ReLU).\n",
        "The data flow will be: input -> gcn1 -> activation -> dropout -> gcn2 -> activation -> dropout -> gcn3."
      ],
      "metadata": {
        "id": "Pz3EgQ9fMECp"
      },
      "id": "Pz3EgQ9fMECp"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip GCN_export.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7q4kTf7Ihp6",
        "outputId": "25aa029e-adad-4922-90f7-9ec012969169"
      },
      "id": "G7q4kTf7Ihp6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/GCN_export.zip\n",
            "   creating: data/\n",
            "   creating: data/cora/\n",
            "  inflating: data/cora/cora.cites    \n",
            "  inflating: data/cora/cora.content  \n",
            "  inflating: data/cora/README        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed63e88",
      "metadata": {
        "id": "2ed63e88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa73fc8b",
      "metadata": {
        "id": "fa73fc8b"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54da9116",
      "metadata": {
        "id": "54da9116"
      },
      "outputs": [],
      "source": [
        "def feature_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3bc9ac",
      "metadata": {
        "id": "4c3bc9ac"
      },
      "outputs": [],
      "source": [
        "def adj_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1)) # Sum each row\n",
        "    r_inv = np.power(rowsum, -1/2).flatten() # Negative square root\n",
        "#     r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv) # Create diagonal matrix\n",
        "\n",
        "    # D^(-1/2).A.D^(-1/2)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    mx = mx.dot(r_mat_inv)\n",
        "    return mx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e762d1",
      "metadata": {
        "id": "73e762d1"
      },
      "outputs": [],
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec661dc1",
      "metadata": {
        "id": "ec661dc1"
      },
      "outputs": [],
      "source": [
        "def load_data(path=\"/content/data/cora/\", dataset=\"cora\"):\n",
        "\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32) # Processing features into a sparse matrix\n",
        "    labels = encode_onehot(idx_features_labels[:, -1]) # one-hot encoding the labels\n",
        "\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # Reading node-ids\n",
        "    idx_map = {j: i for i, j in enumerate(idx)} # Creating index for nodes to map it in adjacency matrix\n",
        "\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32) # Reading edges\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape) # Mapping node-ids in the edge list to the index\n",
        "\n",
        "    # Build adjacency matrix\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # CHECK OUT THE DIFFERENCES BETWEEN csr_matrix (features) and coo_matrix (adj)\n",
        "\n",
        "    # Normalizing features\n",
        "    features = feature_normalize(features)\n",
        "\n",
        "#     # build symmetric adjacency matrix\n",
        "#     adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "#     Normalizing the adjacency matrix after adding self loops\n",
        "    adj = adj_normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    # Setting training, validation, and test range\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    # Converting all matrices into pytorch tensors\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bce67b2",
      "metadata": {
        "id": "5bce67b2"
      },
      "outputs": [],
      "source": [
        "# Function to find accuracy from two tensors\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels) # Get the index of maximum value of 1 dimension and typecast to labels datatype\n",
        "    correct = preds.eq(labels).double() # Convert into double\n",
        "    correct = correct.sum() # Sum correct predictions\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da7e8928",
      "metadata": {
        "id": "da7e8928"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "# Class to define a neural network layer that inherits PyTorch Module\n",
        "# Check out documentaion of the base class 'Module' at:\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "class GraphConvolution(Module):\n",
        "    # Each layer requires no. of input features, no. of output features, and optional bias\n",
        "    def __init__(self, in_feat, out_feat, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "\n",
        "        self.in_features = in_feat\n",
        "        self.out_features = out_feat\n",
        "\n",
        "        # Using Parameter to automatically add weights and bias to learnable parameters\n",
        "        #THIS WILL BE USEFUL ONLY WHEN WE USE Module in the model\n",
        "        self.weight = Parameter(torch.FloatTensor(in_feat, out_feat))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_feat))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # Function to get uniform distribution of weights and bias values\n",
        "    # Can be removed if necessary\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    # Forward function where it actually requires the input data and operations\n",
        "    def forward(self, inp, adj):\n",
        "        # Basically we multiply A.H,W\n",
        "        support = torch.mm(inp, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "\n",
        "        # Adding bias if true\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b147458",
      "metadata": {
        "id": "9b147458"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\"\"\"\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Class to define the model architecture\n",
        "class GCN(nn.Module):\n",
        "    # The model needs no. of input features, no. of hidden units,\n",
        "    # no. of classes, and optional dropout\n",
        "\n",
        "    # NOTE: We use a simply model with one hidden layer\n",
        "        # Architecture will change for deep models\n",
        "        # Ideally, we keep only a few layers in most GNNs\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # Defining one hidden layer and one output layer\n",
        "        self.gcn1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gcn2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    # Similar to GraphConvolution, we give required input data to the forward function\n",
        "    # And specify operations - here it is activation and dropout\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gcn1(x, adj)) # Applying non-linearity on hidden layer 1\n",
        "        # Checkout difference between nn.Dropout() and F.dropout()\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gcn2(x, adj)\n",
        "        return F.log_softmax(x, dim=1) # Applying lograthmic softmax on output layer\n",
        "\n",
        "\"\"\"\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nhid2, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gcn1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gcn2 = GraphConvolution(nhid, nhid2)\n",
        "        self.gcn3 = GraphConvolution(nhid2, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gcn1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.relu(self.gcn2(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gcn3(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "990494b4",
      "metadata": {
        "id": "990494b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d25557b-b50b-4ba8-98e0-e352d1f5c13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-3650411315.py:8: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:644.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "adj, features, labels, train_ids, val_ids, test_ids = load_data()\n",
        "\"\"\"\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "\"\"\"\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nhid2=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "\n",
        "\n",
        "# Using Adam optimizer. Other optimizer can be used too\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=0.01, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2542c4d",
      "metadata": {
        "id": "e2542c4d"
      },
      "outputs": [],
      "source": [
        "# Code for GPU computing\n",
        "\n",
        "# CHANGE THIS CODE TO SUIT YOUR VERSION OF PYTORCH. THE SYNTAX OF THIS COULD VARY SIGNIFICANTLY\n",
        "\n",
        "# If cuda is available, move all data to gpu\n",
        "# And preparing for CUDA operations\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    train_ids.cuda()\n",
        "    val_ids.cuda()\n",
        "    test_ids.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146eb197",
      "metadata": {
        "id": "146eb197"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "\n",
        "    # Evaluating the model in training mode\n",
        "    model.train()\n",
        "    optimizer.zero_grad() # Reseting gradient at each layer to avoid exploding gradient problem\n",
        "    output = model(features, adj)\n",
        "     # Optimizing with nll_loss. Other losses like cross_entropy can also be used\n",
        "    loss_train = F.nll_loss(output[train_ids], labels[train_ids])\n",
        "    acc_train = accuracy(output[train_ids], labels[train_ids])\n",
        "    # backprop and optimize model parameters\n",
        "    # Not needed to specify parameters when using Parameter\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluating validation performance separately\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n",
        "    acc_val = accuracy(output[val_ids], labels[val_ids])\n",
        "\n",
        "\n",
        "    loss_val = F.nll_loss(output[val_ids], labels[val_ids])\n",
        "    acc_val = accuracy(output[val_ids], labels[val_ids])\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b32be098",
      "metadata": {
        "id": "b32be098"
      },
      "outputs": [],
      "source": [
        "# Function to test the model\n",
        "\"\"\"\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[test_ids], labels[test_ids])\n",
        "    acc_test = accuracy(output[test_ids], labels[test_ids])\n",
        "\n",
        "\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(features, adj)\n",
        "        loss_test = F.nll_loss(output[test_ids], labels[test_ids])\n",
        "        acc_test = accuracy(output[test_ids], labels[test_ids])\n",
        "\n",
        "        _, predicted = torch.max(output[test_ids], 1)\n",
        "\n",
        "        # Moving the data to CPU for sklearn metrics calculation\n",
        "        predicted_np = predicted.cpu().numpy()\n",
        "        labels_np = labels[test_ids].cpu().numpy()\n",
        "\n",
        "        precision = precision_score(labels_np, predicted_np, average='weighted')\n",
        "        recall = recall_score(labels_np, predicted_np, average='weighted')\n",
        "        f1 = f1_score(labels_np, predicted_np, average='weighted')\n",
        "\n",
        "        print(f\"Test set results: loss= {loss_test.item():.4f}, accuracy= {acc_test:.4f}, precision= {precision:.4f}, recall= {recall:.4f}, F1 Score= {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a32da91b",
      "metadata": {
        "id": "a32da91b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8553ba-d326-4907-8c0c-3057e8812a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9966 acc_train: 0.2000 loss_val: 2.0142 acc_val: 0.1567 time: 0.1320s\n",
            "Epoch: 0002 loss_train: 1.9771 acc_train: 0.1857 loss_val: 1.9993 acc_val: 0.1567 time: 0.0209s\n",
            "Epoch: 0003 loss_train: 1.9654 acc_train: 0.2000 loss_val: 1.9856 acc_val: 0.1567 time: 0.0206s\n",
            "Epoch: 0004 loss_train: 1.9472 acc_train: 0.2000 loss_val: 1.9729 acc_val: 0.1567 time: 0.0206s\n",
            "Epoch: 0005 loss_train: 1.9269 acc_train: 0.2000 loss_val: 1.9614 acc_val: 0.1567 time: 0.0216s\n",
            "Epoch: 0006 loss_train: 1.9390 acc_train: 0.2000 loss_val: 1.9510 acc_val: 0.1567 time: 0.0209s\n",
            "Epoch: 0007 loss_train: 1.9156 acc_train: 0.2000 loss_val: 1.9410 acc_val: 0.1567 time: 0.0216s\n",
            "Epoch: 0008 loss_train: 1.9065 acc_train: 0.2000 loss_val: 1.9311 acc_val: 0.1567 time: 0.0242s\n",
            "Epoch: 0009 loss_train: 1.8936 acc_train: 0.2000 loss_val: 1.9212 acc_val: 0.1567 time: 0.0249s\n",
            "Epoch: 0010 loss_train: 1.8901 acc_train: 0.2000 loss_val: 1.9110 acc_val: 0.1567 time: 0.0220s\n",
            "Epoch: 0011 loss_train: 1.8716 acc_train: 0.2000 loss_val: 1.9009 acc_val: 0.1567 time: 0.0285s\n",
            "Epoch: 0012 loss_train: 1.8637 acc_train: 0.2000 loss_val: 1.8909 acc_val: 0.1600 time: 0.0217s\n",
            "Epoch: 0013 loss_train: 1.8367 acc_train: 0.2071 loss_val: 1.8814 acc_val: 0.1600 time: 0.0211s\n",
            "Epoch: 0014 loss_train: 1.8147 acc_train: 0.2286 loss_val: 1.8727 acc_val: 0.1633 time: 0.0209s\n",
            "Epoch: 0015 loss_train: 1.8387 acc_train: 0.2071 loss_val: 1.8650 acc_val: 0.1667 time: 0.0210s\n",
            "Epoch: 0016 loss_train: 1.8379 acc_train: 0.2286 loss_val: 1.8581 acc_val: 0.1833 time: 0.0217s\n",
            "Epoch: 0017 loss_train: 1.8532 acc_train: 0.1929 loss_val: 1.8517 acc_val: 0.2100 time: 0.0211s\n",
            "Epoch: 0018 loss_train: 1.8224 acc_train: 0.2786 loss_val: 1.8451 acc_val: 0.2933 time: 0.0206s\n",
            "Epoch: 0019 loss_train: 1.8055 acc_train: 0.2643 loss_val: 1.8385 acc_val: 0.3467 time: 0.0206s\n",
            "Epoch: 0020 loss_train: 1.8205 acc_train: 0.2643 loss_val: 1.8322 acc_val: 0.3533 time: 0.0202s\n",
            "Epoch: 0021 loss_train: 1.8010 acc_train: 0.2643 loss_val: 1.8258 acc_val: 0.3467 time: 0.0214s\n",
            "Epoch: 0022 loss_train: 1.8319 acc_train: 0.2714 loss_val: 1.8193 acc_val: 0.3467 time: 0.0204s\n",
            "Epoch: 0023 loss_train: 1.8108 acc_train: 0.2500 loss_val: 1.8132 acc_val: 0.3467 time: 0.0206s\n",
            "Epoch: 0024 loss_train: 1.7848 acc_train: 0.2500 loss_val: 1.8077 acc_val: 0.3467 time: 0.0211s\n",
            "Epoch: 0025 loss_train: 1.7905 acc_train: 0.3000 loss_val: 1.8023 acc_val: 0.3467 time: 0.0205s\n",
            "Epoch: 0026 loss_train: 1.7539 acc_train: 0.3143 loss_val: 1.7965 acc_val: 0.3500 time: 0.0220s\n",
            "Epoch: 0027 loss_train: 1.7499 acc_train: 0.3357 loss_val: 1.7898 acc_val: 0.3500 time: 0.0205s\n",
            "Epoch: 0028 loss_train: 1.7459 acc_train: 0.2857 loss_val: 1.7822 acc_val: 0.3533 time: 0.0269s\n",
            "Epoch: 0029 loss_train: 1.7299 acc_train: 0.2786 loss_val: 1.7739 acc_val: 0.3533 time: 0.0259s\n",
            "Epoch: 0030 loss_train: 1.7255 acc_train: 0.3071 loss_val: 1.7641 acc_val: 0.3667 time: 0.0200s\n",
            "Epoch: 0031 loss_train: 1.6801 acc_train: 0.3500 loss_val: 1.7532 acc_val: 0.3700 time: 0.0221s\n",
            "Epoch: 0032 loss_train: 1.6828 acc_train: 0.3857 loss_val: 1.7424 acc_val: 0.3733 time: 0.0204s\n",
            "Epoch: 0033 loss_train: 1.6672 acc_train: 0.3357 loss_val: 1.7314 acc_val: 0.3700 time: 0.0200s\n",
            "Epoch: 0034 loss_train: 1.6283 acc_train: 0.3000 loss_val: 1.7196 acc_val: 0.3767 time: 0.0201s\n",
            "Epoch: 0035 loss_train: 1.5958 acc_train: 0.3357 loss_val: 1.7070 acc_val: 0.3767 time: 0.0208s\n",
            "Epoch: 0036 loss_train: 1.6189 acc_train: 0.3071 loss_val: 1.6934 acc_val: 0.3833 time: 0.0205s\n",
            "Epoch: 0037 loss_train: 1.5719 acc_train: 0.3500 loss_val: 1.6785 acc_val: 0.3967 time: 0.0203s\n",
            "Epoch: 0038 loss_train: 1.5499 acc_train: 0.3643 loss_val: 1.6635 acc_val: 0.4133 time: 0.0205s\n",
            "Epoch: 0039 loss_train: 1.5720 acc_train: 0.3429 loss_val: 1.6493 acc_val: 0.4067 time: 0.0214s\n",
            "Epoch: 0040 loss_train: 1.5672 acc_train: 0.3786 loss_val: 1.6351 acc_val: 0.4167 time: 0.0203s\n",
            "Epoch: 0041 loss_train: 1.5272 acc_train: 0.3929 loss_val: 1.6226 acc_val: 0.4133 time: 0.0350s\n",
            "Epoch: 0042 loss_train: 1.4951 acc_train: 0.4357 loss_val: 1.6107 acc_val: 0.4100 time: 0.0211s\n",
            "Epoch: 0043 loss_train: 1.4583 acc_train: 0.3929 loss_val: 1.5996 acc_val: 0.4100 time: 0.0206s\n",
            "Epoch: 0044 loss_train: 1.5214 acc_train: 0.3857 loss_val: 1.5895 acc_val: 0.4100 time: 0.0204s\n",
            "Epoch: 0045 loss_train: 1.4170 acc_train: 0.4000 loss_val: 1.5812 acc_val: 0.4100 time: 0.0199s\n",
            "Epoch: 0046 loss_train: 1.4552 acc_train: 0.4143 loss_val: 1.5739 acc_val: 0.4100 time: 0.0217s\n",
            "Epoch: 0047 loss_train: 1.4004 acc_train: 0.4071 loss_val: 1.5668 acc_val: 0.4167 time: 0.0197s\n",
            "Epoch: 0048 loss_train: 1.4282 acc_train: 0.4000 loss_val: 1.5606 acc_val: 0.4133 time: 0.0203s\n",
            "Epoch: 0049 loss_train: 1.4123 acc_train: 0.4143 loss_val: 1.5546 acc_val: 0.4200 time: 0.0199s\n",
            "Epoch: 0050 loss_train: 1.3964 acc_train: 0.4643 loss_val: 1.5495 acc_val: 0.4200 time: 0.0201s\n",
            "Epoch: 0051 loss_train: 1.3633 acc_train: 0.4643 loss_val: 1.5450 acc_val: 0.4267 time: 0.0237s\n",
            "Epoch: 0052 loss_train: 1.3828 acc_train: 0.4500 loss_val: 1.5398 acc_val: 0.4333 time: 0.0196s\n",
            "Epoch: 0053 loss_train: 1.3205 acc_train: 0.4786 loss_val: 1.5345 acc_val: 0.4367 time: 0.0195s\n",
            "Epoch: 0054 loss_train: 1.3329 acc_train: 0.5143 loss_val: 1.5269 acc_val: 0.4300 time: 0.0245s\n",
            "Epoch: 0055 loss_train: 1.3565 acc_train: 0.4500 loss_val: 1.5199 acc_val: 0.4200 time: 0.0228s\n",
            "Epoch: 0056 loss_train: 1.3182 acc_train: 0.4714 loss_val: 1.5135 acc_val: 0.4233 time: 0.0204s\n",
            "Epoch: 0057 loss_train: 1.3122 acc_train: 0.4714 loss_val: 1.5084 acc_val: 0.4200 time: 0.0204s\n",
            "Epoch: 0058 loss_train: 1.2961 acc_train: 0.4643 loss_val: 1.5037 acc_val: 0.4200 time: 0.0207s\n",
            "Epoch: 0059 loss_train: 1.2880 acc_train: 0.4786 loss_val: 1.5003 acc_val: 0.4400 time: 0.0214s\n",
            "Epoch: 0060 loss_train: 1.2361 acc_train: 0.4929 loss_val: 1.5008 acc_val: 0.4567 time: 0.0211s\n",
            "Epoch: 0061 loss_train: 1.2423 acc_train: 0.5214 loss_val: 1.5021 acc_val: 0.4700 time: 0.0225s\n",
            "Epoch: 0062 loss_train: 1.2931 acc_train: 0.4929 loss_val: 1.5006 acc_val: 0.4733 time: 0.0217s\n",
            "Epoch: 0063 loss_train: 1.2157 acc_train: 0.5143 loss_val: 1.4930 acc_val: 0.4733 time: 0.0205s\n",
            "Epoch: 0064 loss_train: 1.2220 acc_train: 0.5143 loss_val: 1.4832 acc_val: 0.4767 time: 0.0211s\n",
            "Epoch: 0065 loss_train: 1.2102 acc_train: 0.4929 loss_val: 1.4743 acc_val: 0.4867 time: 0.0227s\n",
            "Epoch: 0066 loss_train: 1.2043 acc_train: 0.5214 loss_val: 1.4675 acc_val: 0.4733 time: 0.0210s\n",
            "Epoch: 0067 loss_train: 1.1618 acc_train: 0.5071 loss_val: 1.4625 acc_val: 0.4800 time: 0.0220s\n",
            "Epoch: 0068 loss_train: 1.1707 acc_train: 0.5143 loss_val: 1.4602 acc_val: 0.4833 time: 0.0208s\n",
            "Epoch: 0069 loss_train: 1.1329 acc_train: 0.5500 loss_val: 1.4584 acc_val: 0.4767 time: 0.0213s\n",
            "Epoch: 0070 loss_train: 1.1416 acc_train: 0.5500 loss_val: 1.4547 acc_val: 0.4700 time: 0.0207s\n",
            "Epoch: 0071 loss_train: 1.1652 acc_train: 0.5286 loss_val: 1.4502 acc_val: 0.4833 time: 0.0264s\n",
            "Epoch: 0072 loss_train: 1.0847 acc_train: 0.5571 loss_val: 1.4436 acc_val: 0.4833 time: 0.0219s\n",
            "Epoch: 0073 loss_train: 1.1064 acc_train: 0.5857 loss_val: 1.4352 acc_val: 0.4833 time: 0.0203s\n",
            "Epoch: 0074 loss_train: 1.1224 acc_train: 0.5500 loss_val: 1.4281 acc_val: 0.4833 time: 0.0204s\n",
            "Epoch: 0075 loss_train: 1.0793 acc_train: 0.5786 loss_val: 1.4194 acc_val: 0.4767 time: 0.0213s\n",
            "Epoch: 0076 loss_train: 1.0718 acc_train: 0.5500 loss_val: 1.4126 acc_val: 0.4833 time: 0.0281s\n",
            "Epoch: 0077 loss_train: 1.0558 acc_train: 0.6214 loss_val: 1.4074 acc_val: 0.4867 time: 0.0245s\n",
            "Epoch: 0078 loss_train: 1.0399 acc_train: 0.5857 loss_val: 1.4039 acc_val: 0.4967 time: 0.0201s\n",
            "Epoch: 0079 loss_train: 1.0424 acc_train: 0.5929 loss_val: 1.4005 acc_val: 0.5067 time: 0.0205s\n",
            "Epoch: 0080 loss_train: 1.0569 acc_train: 0.5714 loss_val: 1.3991 acc_val: 0.5000 time: 0.0209s\n",
            "Epoch: 0081 loss_train: 1.0243 acc_train: 0.6000 loss_val: 1.3936 acc_val: 0.4967 time: 0.0224s\n",
            "Epoch: 0082 loss_train: 1.0366 acc_train: 0.6000 loss_val: 1.3857 acc_val: 0.5033 time: 0.0213s\n",
            "Epoch: 0083 loss_train: 0.9886 acc_train: 0.6143 loss_val: 1.3775 acc_val: 0.5133 time: 0.0208s\n",
            "Epoch: 0084 loss_train: 1.0078 acc_train: 0.6000 loss_val: 1.3687 acc_val: 0.5200 time: 0.0208s\n",
            "Epoch: 0085 loss_train: 0.9709 acc_train: 0.6071 loss_val: 1.3606 acc_val: 0.5233 time: 0.0218s\n",
            "Epoch: 0086 loss_train: 0.9953 acc_train: 0.6000 loss_val: 1.3531 acc_val: 0.5333 time: 0.0204s\n",
            "Epoch: 0087 loss_train: 0.9653 acc_train: 0.6429 loss_val: 1.3469 acc_val: 0.5433 time: 0.0200s\n",
            "Epoch: 0088 loss_train: 0.9453 acc_train: 0.6214 loss_val: 1.3445 acc_val: 0.5300 time: 0.0206s\n",
            "Epoch: 0089 loss_train: 0.8774 acc_train: 0.6429 loss_val: 1.3428 acc_val: 0.5267 time: 0.0203s\n",
            "Epoch: 0090 loss_train: 0.9448 acc_train: 0.6000 loss_val: 1.3410 acc_val: 0.5300 time: 0.0205s\n",
            "Epoch: 0091 loss_train: 0.8457 acc_train: 0.6643 loss_val: 1.3432 acc_val: 0.5300 time: 0.0217s\n",
            "Epoch: 0092 loss_train: 0.9304 acc_train: 0.6214 loss_val: 1.3365 acc_val: 0.5300 time: 0.0232s\n",
            "Epoch: 0093 loss_train: 0.8924 acc_train: 0.6929 loss_val: 1.3281 acc_val: 0.5333 time: 0.0201s\n",
            "Epoch: 0094 loss_train: 0.8617 acc_train: 0.7143 loss_val: 1.3197 acc_val: 0.5300 time: 0.0207s\n",
            "Epoch: 0095 loss_train: 0.8724 acc_train: 0.6643 loss_val: 1.3137 acc_val: 0.5367 time: 0.0224s\n",
            "Epoch: 0096 loss_train: 0.8597 acc_train: 0.6500 loss_val: 1.3110 acc_val: 0.5300 time: 0.0197s\n",
            "Epoch: 0097 loss_train: 0.8201 acc_train: 0.6857 loss_val: 1.3120 acc_val: 0.5300 time: 0.0201s\n",
            "Epoch: 0098 loss_train: 0.9122 acc_train: 0.6357 loss_val: 1.3149 acc_val: 0.5300 time: 0.0200s\n",
            "Epoch: 0099 loss_train: 0.8389 acc_train: 0.6786 loss_val: 1.3189 acc_val: 0.5300 time: 0.0199s\n",
            "Epoch: 0100 loss_train: 0.8825 acc_train: 0.6214 loss_val: 1.3127 acc_val: 0.5367 time: 0.0241s\n",
            "Epoch: 0101 loss_train: 0.8697 acc_train: 0.6500 loss_val: 1.3072 acc_val: 0.5400 time: 0.0326s\n",
            "Epoch: 0102 loss_train: 0.8231 acc_train: 0.6643 loss_val: 1.3030 acc_val: 0.5500 time: 0.0213s\n",
            "Epoch: 0103 loss_train: 0.8366 acc_train: 0.6857 loss_val: 1.3010 acc_val: 0.5533 time: 0.0236s\n",
            "Epoch: 0104 loss_train: 0.7865 acc_train: 0.6857 loss_val: 1.2975 acc_val: 0.5500 time: 0.0206s\n",
            "Epoch: 0105 loss_train: 0.8052 acc_train: 0.7000 loss_val: 1.3082 acc_val: 0.5467 time: 0.0202s\n",
            "Epoch: 0106 loss_train: 0.8062 acc_train: 0.7286 loss_val: 1.3254 acc_val: 0.5633 time: 0.0218s\n",
            "Epoch: 0107 loss_train: 0.8476 acc_train: 0.6929 loss_val: 1.3294 acc_val: 0.5667 time: 0.0205s\n",
            "Epoch: 0108 loss_train: 0.8230 acc_train: 0.7143 loss_val: 1.3210 acc_val: 0.5700 time: 0.0211s\n",
            "Epoch: 0109 loss_train: 0.7209 acc_train: 0.7429 loss_val: 1.3079 acc_val: 0.5800 time: 0.0201s\n",
            "Epoch: 0110 loss_train: 0.7060 acc_train: 0.6786 loss_val: 1.2974 acc_val: 0.5733 time: 0.0206s\n",
            "Epoch: 0111 loss_train: 0.8071 acc_train: 0.6714 loss_val: 1.2930 acc_val: 0.5667 time: 0.0216s\n",
            "Epoch: 0112 loss_train: 0.7120 acc_train: 0.7500 loss_val: 1.2940 acc_val: 0.5833 time: 0.0204s\n",
            "Epoch: 0113 loss_train: 0.7266 acc_train: 0.7714 loss_val: 1.2897 acc_val: 0.5867 time: 0.0205s\n",
            "Epoch: 0114 loss_train: 0.7971 acc_train: 0.7143 loss_val: 1.2894 acc_val: 0.5833 time: 0.0203s\n",
            "Epoch: 0115 loss_train: 0.7587 acc_train: 0.7214 loss_val: 1.3104 acc_val: 0.6100 time: 0.0206s\n",
            "Epoch: 0116 loss_train: 0.7837 acc_train: 0.7071 loss_val: 1.3266 acc_val: 0.6033 time: 0.0205s\n",
            "Epoch: 0117 loss_train: 0.7738 acc_train: 0.7286 loss_val: 1.3271 acc_val: 0.6067 time: 0.0196s\n",
            "Epoch: 0118 loss_train: 0.7603 acc_train: 0.7357 loss_val: 1.3096 acc_val: 0.6100 time: 0.0200s\n",
            "Epoch: 0119 loss_train: 0.7074 acc_train: 0.7571 loss_val: 1.2903 acc_val: 0.6067 time: 0.0216s\n",
            "Epoch: 0120 loss_train: 0.7249 acc_train: 0.7286 loss_val: 1.2815 acc_val: 0.5933 time: 0.0209s\n",
            "Epoch: 0121 loss_train: 0.7129 acc_train: 0.7429 loss_val: 1.2826 acc_val: 0.5900 time: 0.0225s\n",
            "Epoch: 0122 loss_train: 0.6932 acc_train: 0.7929 loss_val: 1.2778 acc_val: 0.5900 time: 0.0200s\n",
            "Epoch: 0123 loss_train: 0.7147 acc_train: 0.7429 loss_val: 1.2725 acc_val: 0.6133 time: 0.0217s\n",
            "Epoch: 0124 loss_train: 0.6187 acc_train: 0.7857 loss_val: 1.2927 acc_val: 0.6167 time: 0.0278s\n",
            "Epoch: 0125 loss_train: 0.6796 acc_train: 0.8071 loss_val: 1.3292 acc_val: 0.5933 time: 0.0259s\n",
            "Epoch: 0126 loss_train: 0.6695 acc_train: 0.7071 loss_val: 1.3400 acc_val: 0.5933 time: 0.0197s\n",
            "Epoch: 0127 loss_train: 0.6478 acc_train: 0.7786 loss_val: 1.3237 acc_val: 0.6000 time: 0.0201s\n",
            "Epoch: 0128 loss_train: 0.6561 acc_train: 0.7571 loss_val: 1.3119 acc_val: 0.6067 time: 0.0203s\n",
            "Epoch: 0129 loss_train: 0.6226 acc_train: 0.7786 loss_val: 1.2907 acc_val: 0.6033 time: 0.0201s\n",
            "Epoch: 0130 loss_train: 0.6442 acc_train: 0.7571 loss_val: 1.2818 acc_val: 0.6067 time: 0.0204s\n",
            "Epoch: 0131 loss_train: 0.6224 acc_train: 0.8000 loss_val: 1.2906 acc_val: 0.5833 time: 0.0227s\n",
            "Epoch: 0132 loss_train: 0.7314 acc_train: 0.7500 loss_val: 1.2852 acc_val: 0.5867 time: 0.0205s\n",
            "Epoch: 0133 loss_train: 0.6433 acc_train: 0.7857 loss_val: 1.2788 acc_val: 0.6133 time: 0.0206s\n",
            "Epoch: 0134 loss_train: 0.6623 acc_train: 0.7786 loss_val: 1.2854 acc_val: 0.6200 time: 0.0211s\n",
            "Epoch: 0135 loss_train: 0.5941 acc_train: 0.7929 loss_val: 1.3143 acc_val: 0.5967 time: 0.0208s\n",
            "Epoch: 0136 loss_train: 0.5689 acc_train: 0.7786 loss_val: 1.3456 acc_val: 0.6000 time: 0.0207s\n",
            "Epoch: 0137 loss_train: 0.6775 acc_train: 0.7357 loss_val: 1.3685 acc_val: 0.5967 time: 0.0210s\n",
            "Epoch: 0138 loss_train: 0.5388 acc_train: 0.8286 loss_val: 1.3669 acc_val: 0.5933 time: 0.0206s\n",
            "Epoch: 0139 loss_train: 0.6348 acc_train: 0.7929 loss_val: 1.3435 acc_val: 0.6067 time: 0.0215s\n",
            "Epoch: 0140 loss_train: 0.5347 acc_train: 0.7929 loss_val: 1.3140 acc_val: 0.6133 time: 0.0203s\n",
            "Epoch: 0141 loss_train: 0.6493 acc_train: 0.7786 loss_val: 1.2908 acc_val: 0.6233 time: 0.0226s\n",
            "Epoch: 0142 loss_train: 0.6421 acc_train: 0.8286 loss_val: 1.2785 acc_val: 0.6200 time: 0.0206s\n",
            "Epoch: 0143 loss_train: 0.5784 acc_train: 0.7786 loss_val: 1.2763 acc_val: 0.6067 time: 0.0203s\n",
            "Epoch: 0144 loss_train: 0.5588 acc_train: 0.8000 loss_val: 1.2815 acc_val: 0.6067 time: 0.0208s\n",
            "Epoch: 0145 loss_train: 0.6525 acc_train: 0.7500 loss_val: 1.2932 acc_val: 0.6167 time: 0.0206s\n",
            "Epoch: 0146 loss_train: 0.6744 acc_train: 0.7286 loss_val: 1.2932 acc_val: 0.6167 time: 0.0219s\n",
            "Epoch: 0147 loss_train: 0.6102 acc_train: 0.7929 loss_val: 1.2883 acc_val: 0.6200 time: 0.0237s\n",
            "Epoch: 0148 loss_train: 0.5342 acc_train: 0.8286 loss_val: 1.2814 acc_val: 0.6133 time: 0.0234s\n",
            "Epoch: 0149 loss_train: 0.5930 acc_train: 0.7929 loss_val: 1.2721 acc_val: 0.6233 time: 0.0204s\n",
            "Epoch: 0150 loss_train: 0.5489 acc_train: 0.8000 loss_val: 1.2710 acc_val: 0.6200 time: 0.0216s\n",
            "Epoch: 0151 loss_train: 0.5507 acc_train: 0.7929 loss_val: 1.2706 acc_val: 0.6233 time: 0.0232s\n",
            "Epoch: 0152 loss_train: 0.5600 acc_train: 0.7929 loss_val: 1.2706 acc_val: 0.6233 time: 0.0203s\n",
            "Epoch: 0153 loss_train: 0.6118 acc_train: 0.7714 loss_val: 1.2752 acc_val: 0.6167 time: 0.0208s\n",
            "Epoch: 0154 loss_train: 0.6495 acc_train: 0.7429 loss_val: 1.2805 acc_val: 0.6233 time: 0.0207s\n",
            "Epoch: 0155 loss_train: 0.5374 acc_train: 0.7786 loss_val: 1.2831 acc_val: 0.6200 time: 0.0215s\n",
            "Epoch: 0156 loss_train: 0.6443 acc_train: 0.7571 loss_val: 1.2776 acc_val: 0.6167 time: 0.0208s\n",
            "Epoch: 0157 loss_train: 0.5056 acc_train: 0.8143 loss_val: 1.2687 acc_val: 0.6233 time: 0.0203s\n",
            "Epoch: 0158 loss_train: 0.5932 acc_train: 0.7786 loss_val: 1.2712 acc_val: 0.6267 time: 0.0203s\n",
            "Epoch: 0159 loss_train: 0.4792 acc_train: 0.8357 loss_val: 1.2714 acc_val: 0.6333 time: 0.0208s\n",
            "Epoch: 0160 loss_train: 0.5666 acc_train: 0.8071 loss_val: 1.2675 acc_val: 0.6333 time: 0.0206s\n",
            "Epoch: 0161 loss_train: 0.4697 acc_train: 0.8429 loss_val: 1.2674 acc_val: 0.6367 time: 0.0222s\n",
            "Epoch: 0162 loss_train: 0.6237 acc_train: 0.7643 loss_val: 1.2690 acc_val: 0.6367 time: 0.0206s\n",
            "Epoch: 0163 loss_train: 0.4990 acc_train: 0.8429 loss_val: 1.2678 acc_val: 0.6333 time: 0.0209s\n",
            "Epoch: 0164 loss_train: 0.5435 acc_train: 0.8357 loss_val: 1.2656 acc_val: 0.6367 time: 0.0204s\n",
            "Epoch: 0165 loss_train: 0.5491 acc_train: 0.7857 loss_val: 1.2723 acc_val: 0.6333 time: 0.0206s\n",
            "Epoch: 0166 loss_train: 0.5104 acc_train: 0.8500 loss_val: 1.2846 acc_val: 0.6400 time: 0.0222s\n",
            "Epoch: 0167 loss_train: 0.4577 acc_train: 0.8357 loss_val: 1.2960 acc_val: 0.6333 time: 0.0212s\n",
            "Epoch: 0168 loss_train: 0.4931 acc_train: 0.8214 loss_val: 1.2971 acc_val: 0.6267 time: 0.0207s\n",
            "Epoch: 0169 loss_train: 0.6863 acc_train: 0.7500 loss_val: 1.2907 acc_val: 0.6300 time: 0.0207s\n",
            "Epoch: 0170 loss_train: 0.4665 acc_train: 0.8143 loss_val: 1.2805 acc_val: 0.6333 time: 0.0205s\n",
            "Epoch: 0171 loss_train: 0.5313 acc_train: 0.8286 loss_val: 1.2684 acc_val: 0.6333 time: 0.0215s\n",
            "Epoch: 0172 loss_train: 0.5593 acc_train: 0.8143 loss_val: 1.2634 acc_val: 0.6300 time: 0.0237s\n",
            "Epoch: 0173 loss_train: 0.5248 acc_train: 0.8357 loss_val: 1.2603 acc_val: 0.6233 time: 0.0284s\n",
            "Epoch: 0174 loss_train: 0.5399 acc_train: 0.8286 loss_val: 1.2615 acc_val: 0.6233 time: 0.0214s\n",
            "Epoch: 0175 loss_train: 0.5673 acc_train: 0.8071 loss_val: 1.2666 acc_val: 0.6333 time: 0.0203s\n",
            "Epoch: 0176 loss_train: 0.5008 acc_train: 0.8071 loss_val: 1.2783 acc_val: 0.6433 time: 0.0213s\n",
            "Epoch: 0177 loss_train: 0.4423 acc_train: 0.8357 loss_val: 1.2904 acc_val: 0.6400 time: 0.0203s\n",
            "Epoch: 0178 loss_train: 0.5152 acc_train: 0.8214 loss_val: 1.3144 acc_val: 0.6367 time: 0.0228s\n",
            "Epoch: 0179 loss_train: 0.4868 acc_train: 0.8500 loss_val: 1.3308 acc_val: 0.6300 time: 0.0206s\n",
            "Epoch: 0180 loss_train: 0.5209 acc_train: 0.8214 loss_val: 1.3326 acc_val: 0.6133 time: 0.0219s\n",
            "Epoch: 0181 loss_train: 0.4858 acc_train: 0.8286 loss_val: 1.3314 acc_val: 0.6033 time: 0.0236s\n",
            "Epoch: 0182 loss_train: 0.5879 acc_train: 0.7643 loss_val: 1.3154 acc_val: 0.6033 time: 0.0217s\n",
            "Epoch: 0183 loss_train: 0.4388 acc_train: 0.8571 loss_val: 1.2922 acc_val: 0.6267 time: 0.0211s\n",
            "Epoch: 0184 loss_train: 0.4322 acc_train: 0.8643 loss_val: 1.2783 acc_val: 0.6200 time: 0.0210s\n",
            "Epoch: 0185 loss_train: 0.4987 acc_train: 0.8214 loss_val: 1.2727 acc_val: 0.6333 time: 0.0217s\n",
            "Epoch: 0186 loss_train: 0.5284 acc_train: 0.8071 loss_val: 1.2744 acc_val: 0.6467 time: 0.0207s\n",
            "Epoch: 0187 loss_train: 0.4446 acc_train: 0.8286 loss_val: 1.2718 acc_val: 0.6533 time: 0.0201s\n",
            "Epoch: 0188 loss_train: 0.4540 acc_train: 0.8286 loss_val: 1.2719 acc_val: 0.6367 time: 0.0205s\n",
            "Epoch: 0189 loss_train: 0.5150 acc_train: 0.8000 loss_val: 1.2830 acc_val: 0.6367 time: 0.0208s\n",
            "Epoch: 0190 loss_train: 0.4940 acc_train: 0.8143 loss_val: 1.3077 acc_val: 0.6233 time: 0.0235s\n",
            "Epoch: 0191 loss_train: 0.5276 acc_train: 0.8071 loss_val: 1.3420 acc_val: 0.6100 time: 0.0206s\n",
            "Epoch: 0192 loss_train: 0.4918 acc_train: 0.8429 loss_val: 1.3691 acc_val: 0.6067 time: 0.0210s\n",
            "Epoch: 0193 loss_train: 0.4947 acc_train: 0.8143 loss_val: 1.3668 acc_val: 0.6167 time: 0.0237s\n",
            "Epoch: 0194 loss_train: 0.5356 acc_train: 0.8429 loss_val: 1.3472 acc_val: 0.6167 time: 0.0228s\n",
            "Epoch: 0195 loss_train: 0.4858 acc_train: 0.8500 loss_val: 1.3228 acc_val: 0.6200 time: 0.0212s\n",
            "Epoch: 0196 loss_train: 0.5370 acc_train: 0.8286 loss_val: 1.3049 acc_val: 0.6267 time: 0.0217s\n",
            "Epoch: 0197 loss_train: 0.4171 acc_train: 0.8571 loss_val: 1.3004 acc_val: 0.6333 time: 0.0209s\n",
            "Epoch: 0198 loss_train: 0.5308 acc_train: 0.7786 loss_val: 1.3004 acc_val: 0.6400 time: 0.0209s\n",
            "Epoch: 0199 loss_train: 0.4347 acc_train: 0.8357 loss_val: 1.3017 acc_val: 0.6300 time: 0.0207s\n",
            "Epoch: 0200 loss_train: 0.4853 acc_train: 0.8071 loss_val: 1.3027 acc_val: 0.6367 time: 0.0239s\n",
            "\n",
            "Optimization finished...\n",
            "Total time elapsed: 4.4395s\n",
            "Test set results: loss= 1.3053, accuracy= 0.5720, precision= 0.5937, recall= 0.5720, F1 Score= 0.5667\n"
          ]
        }
      ],
      "source": [
        "t_total = time.time()\n",
        "\n",
        "for epoch in range(200):\n",
        "    train(epoch)\n",
        "\n",
        "print()\n",
        "print('Optimization finished...')\n",
        "print('Total time elapsed: {:.4f}s'.format(time.time() - t_total))\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af85139",
      "metadata": {
        "id": "1af85139"
      },
      "source": [
        "###"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}